{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정의\n",
    "* LLM 모델을 사용하여 질문을 통하여 얻은 답변을 평가하는 시간을 가져보겠습니다.\n",
    "\n",
    "## 평가 방법\n",
    "1. 사람\n",
    "2. 모델\n",
    "3. 코드\n",
    "\n",
    "## 평가 기준\n",
    "* LLM이 어느 정도 성능을 가진 모델인지 정량 or 정성적으로 확인하는 객관적인 지표\n",
    "\n",
    "## LLM 평가 지표는 존재\n",
    "\n",
    "1. MMLU (Massive Multitask Language Understanding) - github, paper\n",
    "  - 여러 분야 테스트하는 객관식 시험\n",
    "  - 참고로 MMLU (5 shot)의 경우 5개의 질문/정답 쌍이 Prompt로 주어졌다는 뜻\n",
    "  - 깃헙: https://github.com/hendrycks/test\n",
    "  - 논문: https://arxiv.org/pdf/2009.03300\n",
    "2. ARC (Abstraction and Reasoning Corpus) - github, paper\n",
    "  - 2차원 pixels grid 주고 특정 문제 해결 ex. 패턴 주고 일부 비워두고 어떤 색깔로 칠 할지 맞추는 문제\n",
    "  - 깃헙: https://github.com/fchollet/ARC\n",
    "  - 논문: https://arxiv.org/abs/1911.01547\n",
    "3. HellaSwag - website, paper\n",
    "  - 문장들 주고 이어지는 마지막 문장들로 가장 적합한 문장들 4개 중 하나 고르는 문제\n",
    "  - 웹사이트: https://rowanzellers.com/hellaswag/\n",
    "  - 논문: https://arxiv.org/pdf/1905.07830\n",
    "4. TruthfulQA - github, agit\n",
    "  - 할루시네이션 측정용 데이터셋이고 주어진 문제에 대한 Accuracy 측정 (문제 유형은 객관식 MC 외에 더 있음)\n",
    "  - 깃헙: https://github.com/sylinrl/TruthfulQA\n",
    "  - 논문: https://arxiv.org/abs/2109.07958\n",
    "\n",
    "\n",
    "위 지표들 외에도 수학, 코딩 능력 등을 확인하는 지표들도 존재함\n",
    "\n",
    "## 태스크에 적합한 평가 지표 설정이 필요한 이유\n",
    "- 실사용 입장에선 사실 위 지표들간의 미미한 우위는 의미 없고 특정 태스크 점수가 중요\n",
    "  - 예를 들어, 최근에 나온 Mistral 모델이 라마2 모델보다 지표 상으로는 더 우월함\n",
    "  - 막상 한글 Q&A 태스크에서는 반대인 상황이라 실서비스 용도로는 후자의 성능이 더 좋은 케이스가 좀 더 많은 편\n",
    "- 즉, 위 지표가 더 높다고 특정 태스크에서 무조건 더 높은건 아님\n",
    "  - **태스크에 적합한 평가 지표 설정이 필요한 이유!**\n",
    "  - 범용 모델을 평가해야 하다보니 대표적인 지표들을 선정한 것\n",
    "\n",
    "### 1. 사람을 활용한 평가\n",
    "1. 전문가 평가(주관적)\n",
    "2. 블라인드 평가\n",
    "3. 명확한 기준\n",
    "4. 더 좋은 답변을 선택하는 테스트\n",
    "* https://chat.lmsys.org/\n",
    "* https://chat.lmsys.org/?leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델을 활용한 평가\n",
    "1. GPT-4 강한 LLM을 통해 평가하는 방법 (가장 유사하다)\n",
    "2. G-Eval 논문\n",
    "    * https://arxiv.org/abs/2303.16634\n",
    "3. MT-Bench 논문\n",
    "    * https://arxiv.org/abs/2306.05685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai 호출\n",
    "!pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API 호출\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"OPENAI_API키를 입력하세요\"\n",
    "\n",
    "# client 호출\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 숲은 초록색인가요?\n"
     ]
    }
   ],
   "source": [
    "# 질문\n",
    "question = \"왜 숲은 초록색인가요?\"\n",
    "\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "숲이 초록색인 이유는 주로 나무의 잎이나 식물들이 엽록소라는 녹색 색소를 함유하고 있기 때문입니다. 엽록소는 빛을 흡수하여 에너지를 생산하는 역할을 하며, 이 과정에서 초록색을 반사하기 때문에 숲은 우리 눈에 초록색으로 보입니다. 또한, 숲의 식물들이 서로 경쟁하며 자라는 과정에서 초록색이 우세해지게 되어 숲이 전체적으로 초록색으로 보이게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# gpt-3.5 모델을 통해 질문 후, 답변(answer3_5)을 얻음\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=[{'role': 'user', 'content': question}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "answer3_5 = completion.choices[0].message.content\n",
    "print(answer3_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "숲이 초록색으로 보이는 주된 이유는 나무와 식물의 잎에 포함된 엽록소(chlorophyll) 때문입니다. 엽록소는 식물이 광합성을 하는 데 필수적인 역할을 합니다. 광합성 과정에서 식물은 태양광을 에너지로 변환하여 탄수화물을 만들고, 이 과정에서 산소를 방출합니다.\n",
      "\n",
      "엽록소는 태양광의 빨간색과 파란색 파장을 흡수하고, 초록색 파장을 반사합니다. 우리 눈은 반사된 빛을 감지하기 때문에, 이 반사된 초록색 빛이 우리에게 식물의 잎이 초록색으로 보이게 만듭니다. 숲이 많은 나무와 식물로 이루어져 있기 때문에, 우리는 숲을 초록색으로 인식하게 됩니다.\n",
      "\n",
      "또한, 다른 색소들도 잎에서 발견될 수 있지만, 엽록소가 가장 우세하기 때문에 초록색이 가장 두드러지게 나타납니다. 가을철에는 일부 나무들이 엽록소를 분해하고 다른 색소들이 더 잘 보이게 되면서 잎이 노란색, 주황색, 빨간색으로 변하는 것을 볼 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# gpt-4 모델을 통해 질문 후, 답변(answer4)을 얻음\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4-1106-preview', # 최근 모델\n",
    "    messages=[{'role': 'user', 'content': question}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "answer4 = completion.choices[0].message.content\n",
    "print(answer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
      "AI assistants to the user question displayed below. You should choose the assistant that\n",
      "follows the user’s instructions and answers the user’s question better. Your evaluation\n",
      "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
      "and level of detail of their responses. Begin your evaluation by comparing the two\n",
      "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
      "order in which the responses were presented does not influence your decision. Do not allow\n",
      "the length of the responses to influence your evaluation. Do not favor certain names of\n",
      "the assistants. Be as objective as possible. After providing your explanation, output your\n",
      "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
      "if assistant B is better, and \"[[C]]\" for a tie.\n",
      "\n",
      "[User Question]\n",
      "왜 숲은 초록색인가요?\n",
      "\n",
      "[The Start of Assistant A’s Answer]\n",
      "숲이 초록색인 이유는 주로 나무의 잎이나 식물들이 엽록소라는 녹색 색소를 함유하고 있기 때문입니다. 엽록소는 빛을 흡수하여 에너지를 생산하는 역할을 하며, 이 과정에서 초록색을 반사하기 때문에 숲은 우리 눈에 초록색으로 보입니다. 또한, 숲의 식물들이 서로 경쟁하며 자라는 과정에서 초록색이 우세해지게 되어 숲이 전체적으로 초록색으로 보이게 됩니다.\n",
      "[The End of Assistant A’s Answer]\n",
      "\n",
      "[The Start of Assistant B’s Answer]\n",
      "숲이 초록색으로 보이는 주된 이유는 나무와 식물의 잎에 포함된 엽록소(chlorophyll) 때문입니다. 엽록소는 식물이 광합성을 하는 데 필수적인 역할을 합니다. 광합성 과정에서 식물은 태양광을 에너지로 변환하여 탄수화물을 만들고, 이 과정에서 산소를 방출합니다.\n",
      "\n",
      "엽록소는 태양광의 빨간색과 파란색 파장을 흡수하고, 초록색 파장을 반사합니다. 우리 눈은 반사된 빛을 감지하기 때문에, 이 반사된 초록색 빛이 우리에게 식물의 잎이 초록색으로 보이게 만듭니다. 숲이 많은 나무와 식물로 이루어져 있기 때문에, 우리는 숲을 초록색으로 인식하게 됩니다.\n",
      "\n",
      "또한, 다른 색소들도 잎에서 발견될 수 있지만, 엽록소가 가장 우세하기 때문에 초록색이 가장 두드러지게 나타납니다. 가을철에는 일부 나무들이 엽록소를 분해하고 다른 색소들이 더 잘 보이게 되면서 잎이 노란색, 주황색, 빨간색으로 변하는 것을 볼 수 있습니다.\n",
      "[The End of Assistant B’s Answer]\n"
     ]
    }
   ],
   "source": [
    "# 평가 프롬프트에 넣어보자\n",
    "# 평가 프롬프트 출처: MT-Bench 논문 https://arxiv.org/pdf/2306.05685.pdf (A. Prompt Templates Figure 5)\n",
    "\n",
    "prompt = f\"\"\"[System]\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the user’s instructions and answers the user’s question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "[The Start of Assistant A’s Answer]\n",
    "{answer3_5}\n",
    "[The End of Assistant A’s Answer]\n",
    "\n",
    "[The Start of Assistant B’s Answer]\n",
    "{answer4}\n",
    "[The End of Assistant B’s Answer]\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the responses from Assistant A and Assistant B, both assistants provide accurate and relevant explanations for why forests appear green, focusing on the role of chlorophyll in plant leaves. However, there are differences in the depth and detail of their explanations.\n",
      "\n",
      "Assistant A's response is concise and correctly identifies chlorophyll as the reason for the green color, mentioning the absorption of light and the reflection of green light. It also touches on the competitive aspect of plant growth in forests contributing to the predominance of green.\n",
      "\n",
      "Assistant B's response, on the other hand, goes into greater detail about the process of photosynthesis and the specific wavelengths of light absorbed and reflected by chlorophyll. This assistant also expands on how chlorophyll's dominance over other pigments in leaves leads to the green color, and further explains the seasonal color changes in leaves, which adds depth and educational value to the answer.\n",
      "\n",
      "In terms of creativity, both responses stick to scientific explanations and do not venture into creative interpretations, which is appropriate for the factual nature of the question.\n",
      "\n",
      "Overall, Assistant B provides a more detailed and comprehensive explanation that not only addresses the primary question but also enriches the user's understanding of related phenomena (like seasonal changes in leaf color). This makes Assistant B's response more helpful and informative.\n",
      "\n",
      "Final verdict: [[B]]\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "# 최신 모델 'gpt-4-turbo-2024-04-09'에 위 prompt를 넣어보자.\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4-turbo-2024-04-09', # 최신 모델\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "영어 해석본을 번역해서 봐보자.\n",
    "\n",
    "보조자 A와 보조자 B의 응답을 비교하면서 두 보조자 모두 식물 잎에서 엽록소의 역할에 초점을 맞춰 숲이 녹색으로 나타나는 이유에 대해 정확하고 관련성 있는 설명을 제공합니다. 그러나 설명의 깊이와 세부사항에는 차이가 있습니다.\n",
    "\n",
    "보조자 A의 반응은 간결하며 빛의 흡수와 녹색 빛의 반사를 언급하면서 녹색의 원인이 엽록소임을 정확하게 식별합니다. 또한 녹색의 우세에 기여하는 숲의 식물 성장의 경쟁적 측면을 다루고 있습니다.\n",
    "\n",
    "반면 보조자 B의 반응은 광합성 과정과 엽록소에 의해 흡수되고 반사되는 빛의 특정 파장에 대해 더 자세히 설명합니다. 이 보조원은 또한 잎의 다른 색소에 대한 엽록소의 지배력이 어떻게 녹색을 띠게 되는지에 대해 확장하고 잎의 계절적 색상 변화에 대해 더 자세히 설명하여 답변에 깊이와 교육적 가치를 더합니다.\n",
    "\n",
    "창의성 측면에서 두 응답 모두 과학적인 설명을 고수하고 질문의 사실적 성격에 적합한 창의적 해석을 시도하지 않습니다.\n",
    "\n",
    "전반적으로 보조 도구 B는 기본 질문을 해결할 뿐만 아니라 관련 현상(예: 잎 색깔의 계절적 변화)에 대한 사용자의 이해를 풍부하게 하는 보다 자세하고 포괄적인 설명을 제공합니다. 이렇게 하면 보조자 B의 응답이 더 유용하고 유익해집니다.\n",
    "\n",
    "최종 평결: [[B]]\n",
    "\n",
    "결론적으로 B예시가 더 좋은 예시라고 평가했다.\n",
    "```\n",
    "\n",
    "1. 평가 프롬프트에 'gpt-3.5'와 'gpt-4'의 질문한 답변을 넣는다.\n",
    "2. 넣은 '평가 프롬프트'를 최신 gpt모델 'gpt-4-turbo-2024-04-09'에 평가한다.\n",
    "3. 결론적으로 B예시가 더 좋은 예시라고 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 코드를 활용한 평가\n",
    "1. Recall, Accuracy, Precision\n",
    "2. BLEU\n",
    "3. ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사람, 모델, 코드를 활용한 평가의 각각 장단점\n",
    "### 1. 사람을 활용한 평가\n",
    "- 장점\n",
    "1. 복잡한 맥락과 뉘앙스를 사람은 이해할 수 있기 때문에, 심층적 이해와 맥락을 평가하는데 유리함\n",
    "2. 통제된 환경일 경우, 사람이 직접 평가한 방법이라 신뢰를 할 수 있음\n",
    "3. 실제 사용해보고 경험과 만족도를 반영할 수 있어, 사용자 중심의 평가 가능\n",
    "\n",
    "- 단점\n",
    "1. 다수의 의한 노이즈 발생 가능성이 있음\n",
    "2. 전문가가 아닐시 정확성이 떨어짐\n",
    "3. 시간과 비용이 높을 수 있음\n",
    "### 2. 모델을 활용한 평가\n",
    "-장점\n",
    "1. 정량적 지표를 통해 객관적으로 성능을 평가할 수 있음\n",
    "2. 자동으로 대규모 데이터셋을 평가할 수 있어, 시간과 비용을 절약할 수 있음\n",
    "3. 평가의 일관성과 신뢰성을 높일 수 있음\n",
    "\n",
    "- 단점\n",
    "1. 모델은 복잡한 맥락이나 뉘앙스를 완벽히 이해하지 못할 수 있음\n",
    "2. 편향이 평가 결과에 미칠 수 있음\n",
    "### 3. 코드를 활용한 평가\n",
    "- 장점\n",
    "1. 무료로 평가할 수 있음\n",
    "2. 신속하고 정확하게 평가할 수 있음\n",
    "3. 복잡한 계산 가능\n",
    "\n",
    "- 단점\n",
    "1. 신뢰성이 떨어질 수 있음\n",
    "2. 기술적 제약 발생\n",
    "\n",
    "## 요약\n",
    "1. 태스크에 적합한 사람이 평가하는 방법이 제일 좋음\n",
    "2. 현실적으로 사람이 평가하는 방법은 쉽지 않아, 모델이 평가하는 방법도 충분히 사용함\n",
    "3. 코드를 활용한 평가를 권장, 현실적으로 고려하는 점이 '비용'이기 때문에\n",
    "4. 가장 이상적인 평가 방법은 정량적 + 정성적 평가 모두 하는 법\n",
    "5. 최종 지표는 사용자의 피드백\n",
    "\n",
    "## 정리\n",
    "- 평가 기준은 총 3가지\n",
    "1. 사람을 활용한 평가\n",
    "2. 모델을 활용한 평가\n",
    "3. 코드를 활용한 평가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
